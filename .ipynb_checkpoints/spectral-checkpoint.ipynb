{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A 10-layer Model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "NUM_LAYER = 200\n",
    "ACT = 'Tanh'\n",
    "INIT = 'XavierInit'\n",
    "MODE = 'NoShortcut'\n",
    "LOGGING_BATCH = 100\n",
    "SCALE = 1.0\n",
    "n_epochs = 20\n",
    "LEARNING_RATE=1e-1\n",
    "BatchNorm = False\n",
    "INIT_GAMMA=0.3\n",
    "GradientClipping = False\n",
    "WeightClipping = False\n",
    "WeightNormalization = False\n",
    "DataNormalization = False\n",
    "WeightRandomize = False\n",
    "delta_p = 1e-1\n",
    "COMMENT = ''\n",
    "SaveJacobian = False\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import axis, ticker\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "(train_images , train_labels) , (test_images , test_labels) = mnist.load_data()\n",
    "\n",
    "def data_soup (training_data , test_data) :\n",
    "    train_images , test_images = training_data [0] , test_data [0]\n",
    "    train_labels , test_labels = training_data [1] , test_data [1]\n",
    "    # flatten\n",
    "    train_images = train_images.reshape ((len (train_images) , 28 * 28))\n",
    "    test_images = test_images.reshape ((len (test_images) , 28 * 28))\n",
    "\n",
    "    # normalization\n",
    "    train_images = train_images.astype ('float32') / 255\n",
    "    test_images = test_images.astype ('float32') / 255\n",
    "\n",
    "    \n",
    "    if DataNormalization:\n",
    "        train_mean = train_images.mean()\n",
    "        train_std = train_images.std()\n",
    "        train_images = (train_images - train_mean)/train_std*0.1\n",
    "        test_images = (test_images - train_mean)/train_std*0.1\n",
    "    \n",
    "\n",
    "    # One hot encoding\n",
    "    train_labels = to_categorical (train_labels)\n",
    "    test_labels = to_categorical (test_labels)\n",
    "\n",
    "    return (train_images , train_labels) , (test_images , test_labels)\n",
    "\n",
    "(train_images , train_labels) , (test_images , test_labels) = data_soup ((train_images , train_labels) ,\n",
    "                                                                         (test_images , test_labels))\n",
    "\n",
    "X = tf.placeholder(dtype = tf.float32, shape = (None, 28*28), name = 'X_inputs')\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = (None, 10), name = 'Y_inputs')\n",
    "isTrain = tf.placeholder(dtype = tf.bool, shape=())\n",
    "\n",
    "NUM_HIDDEN = 500\n",
    "# SMALL_INIT = 1.414e-2\n",
    "# LARGE_INIT = 0.07747\n",
    "SMALL_INIT = 1e-2\n",
    "LARGE_INIT = 2e-1\n",
    "\n",
    "small_W_list = [\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [28*28 , NUM_HIDDEN], stddev = SMALL_INIT),\n",
    "        trainable = True, name='small_W0')\n",
    "]\n",
    "small_W_list.extend([\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , NUM_HIDDEN], stddev = SMALL_INIT),\n",
    "        trainable = True, name='small_W%d'%i)\n",
    "    for i in range(1, NUM_LAYER-1)\n",
    "])\n",
    "small_W_list.append(\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , 10], stddev = SMALL_INIT),\n",
    "        trainable = True, name='small_W%d'%(NUM_LAYER-1))\n",
    ")\n",
    "\n",
    "large_W_list = [\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [28*28 , NUM_HIDDEN], stddev = LARGE_INIT),\n",
    "        trainable = True, name='large_W0')\n",
    "]\n",
    "large_W_list.extend([\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , NUM_HIDDEN], stddev = LARGE_INIT),\n",
    "        trainable = True, name='large_W%d'%i)\n",
    "    for i in range(1, NUM_LAYER-1)\n",
    "])\n",
    "large_W_list.append(\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , 10], stddev = LARGE_INIT),\n",
    "        trainable = True, name='large_W%d'%(NUM_LAYER-1))\n",
    ")\n",
    "\n",
    "xavier_W_list = [\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [28*28 , NUM_HIDDEN], stddev = 1./np.sqrt(28*28)),\n",
    "        trainable = True, name='xavier_W0')\n",
    "]\n",
    "xavier_W_list.extend([\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , NUM_HIDDEN], stddev = 1./np.sqrt(NUM_HIDDEN)),\n",
    "        trainable = True, name='xavier_W%d'%i)\n",
    "    for i in range(1, NUM_LAYER-1)\n",
    "])\n",
    "xavier_W_list.append(\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , 10], stddev = 1./np.sqrt(NUM_HIDDEN)),\n",
    "        trainable = True, name='xavier_W%d'%(NUM_LAYER-1))\n",
    ")\n",
    "\n",
    "invDep_W_list = [\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [28*28 , NUM_HIDDEN], stddev = 1./np.sqrt(28*28*NUM_LAYER)),\n",
    "        trainable = True, name='invDep_W0')\n",
    "]\n",
    "invDep_W_list.extend([\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , NUM_HIDDEN], stddev = 1./np.sqrt(NUM_HIDDEN*NUM_LAYER)),\n",
    "        trainable = True, name='invDep_W%d'%i)\n",
    "    for i in range(1, NUM_LAYER-1)\n",
    "])\n",
    "invDep_W_list.append(\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , 10], stddev = 1./np.sqrt(NUM_HIDDEN*NUM_LAYER)),\n",
    "        trainable = True, name='invDep_W%d'%(NUM_LAYER-1))\n",
    ")\n",
    "\n",
    "orth_W_list = [\n",
    "    tf.Variable(\n",
    "        initial_value = tf.convert_to_tensor(ortho_group.rvs(dim=28*28)[:, 0:NUM_HIDDEN], np.float32),\n",
    "        trainable = True, name='orth_W0')\n",
    "]\n",
    "orth_W_list.extend([\n",
    "    tf.Variable(\n",
    "        initial_value = tf.convert_to_tensor(ortho_group.rvs(dim=NUM_HIDDEN), np.float32),\n",
    "        trainable = True, name='orth_W%d'%i)\n",
    "    for i in range(1, NUM_LAYER-1)\n",
    "])\n",
    "orth_W_list.append(\n",
    "    tf.Variable(\n",
    "        initial_value = tf.convert_to_tensor(ortho_group.rvs(dim=NUM_HIDDEN)[:, 0:10], np.float32),\n",
    "        trainable = True, name='orth_W%d'%(NUM_LAYER-1))\n",
    ")\n",
    "\n",
    "recursive_W_list = [\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [28*28 , NUM_HIDDEN], stddev = np.sqrt(1./784)),\n",
    "        trainable = True, name='recursive_W0')\n",
    "]\n",
    "recursive_W_list.extend([\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , NUM_HIDDEN], stddev = np.sqrt((1.-SCALE**2)/NUM_HIDDEN)),\n",
    "        trainable = True, name='recursive_W%d'%i)\n",
    "    for i in range(1, NUM_LAYER-1)\n",
    "])\n",
    "recursive_W_list.append(\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , 10], stddev = 1./np.sqrt(NUM_HIDDEN)),\n",
    "        trainable = True, name='recursive_W%d'%(NUM_LAYER-1))\n",
    ")\n",
    "\n",
    "kaiming_W_list = [\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [28*28 , NUM_HIDDEN], stddev = 1./np.sqrt(28*28/2)),\n",
    "        trainable = True, name='kaiming_W0')\n",
    "]\n",
    "kaiming_W_list.extend([\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , NUM_HIDDEN], stddev = 1./np.sqrt(NUM_HIDDEN/2)),\n",
    "        trainable = True, name='kaiming_W%d'%i)\n",
    "    for i in range(1, NUM_LAYER-1)\n",
    "])\n",
    "kaiming_W_list.append(\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , 10], stddev = 1./np.sqrt(NUM_HIDDEN/2)),\n",
    "        trainable = True, name='kaiming_W%d'%(NUM_LAYER-1))\n",
    ")\n",
    "\n",
    "vd_xavier_W_list = [\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [28*28 , NUM_HIDDEN], stddev = 1./np.sqrt(28*28)),\n",
    "        trainable = True, name='vdxavier_W0')\n",
    "]\n",
    "vd_xavier_W_list.extend([\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , NUM_HIDDEN], stddev = 1./np.sqrt(NUM_HIDDEN if i <= 2 else NUM_HIDDEN+NUM_HIDDEN*(i-2)*SCALE)),\n",
    "        trainable = True, name='vdxavier_W%d'%i)\n",
    "    for i in range(1, NUM_LAYER-1)\n",
    "])\n",
    "vd_xavier_W_list.append(\n",
    "    tf.Variable(\n",
    "        initial_value = tf.random_normal(shape = [NUM_HIDDEN , 10], stddev = 1./np.sqrt(NUM_HIDDEN+NUM_HIDDEN*(NUM_LAYER-3)*SCALE)),\n",
    "        trainable = True, name='vdxavier_W%d'%(NUM_LAYER-1))\n",
    ")\n",
    "\n",
    "\n",
    "weight_dict = {'SmallInit':small_W_list, 'LargeInit':large_W_list, 'XavierInit':xavier_W_list,  \n",
    "               'VDXavierInit':vd_xavier_W_list, 'KaimingInit':kaiming_W_list, 'RecursiveInit':recursive_W_list,\n",
    "               'InverseDepthInit':invDep_W_list, 'OrthogonalInit':orth_W_list}\n",
    "\n",
    "sigmoid = tf.nn.sigmoid\n",
    "tanh = tf.nn.tanh\n",
    "relu = tf.nn.relu\n",
    "\n",
    "act_dict = {'Sigmoid':sigmoid, 'Tanh':tanh, 'Relu':relu, 'Iden':lambda x, name: x}\n",
    "\n",
    "\n",
    "act = act_dict[ACT]\n",
    "W_list = weight_dict[INIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"Recursive\":\n",
    "    h = tf.matmul(X, W_list[0], name='h0')\n",
    "    h_list = [h]\n",
    "    a = act(h, name='act0')\n",
    "    a_list = [a]\n",
    "    for layer_index in range(1, NUM_LAYER-1):\n",
    "        h = tf.matmul(a, W_list[layer_index], name='h%d'%layer_index)\n",
    "        if BatchNorm:\n",
    "            h_norm = tf.layers.batch_normalization(sum(h_list[0:-1])*SCALE+h, training=isTrain, gamma_initializer=tf.constant_initializer(INIT_GAMMA))\n",
    "        else:\n",
    "            h_norm = h_list[-1]*SCALE+h\n",
    "        a = act(h_norm, name='act%d'%layer_index)\n",
    "        h_list.append(h_norm)\n",
    "        a_list.append(a)\n",
    "\n",
    "elif MODE == \"AllToLast\":\n",
    "    h = tf.matmul(X, W_list[0], name='h0')\n",
    "    h_list = [h]\n",
    "    a = act(h, name='act0')\n",
    "    a_list = [a]\n",
    "    for layer_index in range(1, NUM_LAYER-1):\n",
    "        h = tf.matmul(a, W_list[layer_index], name='h%d'%layer_index)\n",
    "        h_list.append(h)\n",
    "        if layer_index == NUM_LAYER-2:\n",
    "            h_norm = sum(h_list) if not BatchNorm else tf.layers.batch_normalization(sum(h_list), training=isTrain, gamma_initializer=tf.constant_initializer(INIT_GAMMA))\n",
    "        else:\n",
    "            h_norm = h if not BatchNorm else tf.layers.batch_normalization(sum(h_list), training=isTrain, gamma_initializer=tf.constant_initializer(INIT_GAMMA))\n",
    "        a = act(h_norm, name='act%d'%layer_index)\n",
    "        a_list.append(a)\n",
    "        \n",
    "elif MODE == \"NoShortcut\":\n",
    "    h_list = []\n",
    "    a_list = []\n",
    "    a = X\n",
    "    for layer_index in range(NUM_LAYER-1):\n",
    "        h = tf.matmul(a, W_list[layer_index], name='h%d'%layer_index)\n",
    "        h_list.append(h)\n",
    "        h_mean, h_var = tf.nn.moments(h, axes=0)\n",
    "        if not BatchNorm:\n",
    "            h_norm = h\n",
    "        else:\n",
    "            h_norm = tf.layers.batch_normalization(h, training=isTrain, gamma_initializer=tf.constant_initializer(INIT_GAMMA))\n",
    "        a = act(h_norm, name='act%d'%layer_index)\n",
    "        a_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_gram_schmidt(vectors, alpha=1.0):\n",
    "    # add batch dimension for matmul\n",
    "    basis = tf.expand_dims(vectors[0,:]/tf.norm(vectors[0,:]),0)\n",
    "    for i in range(1,vectors.get_shape()[0].value):\n",
    "        v = vectors[i,:]\n",
    "        # add batch dimension for matmul\n",
    "        v = tf.expand_dims(v,0) \n",
    "        w = v - alpha*tf.matmul(tf.matmul(v, tf.transpose(basis)), basis)\n",
    "         # I assume that my matrix is close to orthogonal\n",
    "        basis = tf.concat([basis, w/tf.norm(w)],axis=0)\n",
    "    return basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = tf.nn.softmax(tf.matmul(a, W_list[-1]))\n",
    "loss = -tf.reduce_mean(Y * tf.log(Y_pred), name = 'loss')\n",
    "correct_prediction = tf.equal(tf.argmax(Y_pred, axis = 1), tf.argmax(Y, axis = 1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, dtype = tf.float32))\n",
    "\n",
    "lr = tf.Variable(LEARNING_RATE, name=\"learning_rate\")\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=lr, name='GradientDescent')\n",
    "# opt = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9, name='GradientDescent')\n",
    "\n",
    "gradients_list = opt.compute_gradients(loss, W_list)\n",
    "y_gradients_list = opt.compute_gradients(loss, h_list)\n",
    "\n",
    "if GradientClipping:\n",
    "    gradients_list = [\n",
    "        (\n",
    "            tf.cond(\n",
    "                tf.reduce_max(tf.abs(g))>1.0,\n",
    "                lambda: g/tf.reduce_max(tf.abs(g)),\n",
    "                lambda: g),\n",
    "            v\n",
    "        ) \n",
    "        for g, v in gradients_list\n",
    "    ]\n",
    "update_weights = opt.apply_gradients(gradients_list)\n",
    "if WeightRandomize:\n",
    "    randomize_weight = [\n",
    "        tf.assign(w, w+tf.random_normal(w.shape, mean=0.0, stddev=0.3)\n",
    "        ) for w in W_list\n",
    "    ]\n",
    "    update_weights = tf.group(update_weights, *randomize_weight)\n",
    "if WeightClipping:\n",
    "    weight_clipping = [tf.assign(w, tf.cond(\n",
    "        tf.reduce_max(tf.abs(w))>1.0,\n",
    "        lambda: w/tf.reduce_max(tf.abs(w)),\n",
    "        lambda: w\n",
    "    )) for w in W_list]\n",
    "    update_weights = tf.group(update_weights, *weight_clipping)\n",
    "\n",
    "if WeightNormalization:\n",
    "    target_weight_std = [1./np.sqrt(28*28)] + [1./np.sqrt(NUM_HIDDEN)]*(NUM_LAYER-1)\n",
    "    weight_norm_list = [\n",
    "        # tf.assign(w, target_weight_std[w_i]*w/tf.sqrt(tf.nn.moments(w, axes=[0, 1])[1]))\n",
    "        # tf.assign(w,\n",
    "        #     target_weight_std[w_i] * (w-tf.nn.moments(w, axes=[0])[0]) / tf.sqrt(tf.nn.moments(w, axes=[0])[1])\n",
    "        # )\n",
    "        tf.assign(w, tf_gram_schmidt(w, alpha=1.0))\n",
    "        for w_i, w in enumerate(W_list)\n",
    "    ]\n",
    "    update_weights = tf.group(update_weights, *weight_norm_list)\n",
    "\n",
    "    \n",
    "'''loss_summary = tf.summary.scalar(name=\"loss_value_summary\", tensor=loss)\n",
    "acc_summary = tf.summary.scalar(name=\"accuracy_summary\", tensor=acc)\n",
    "weight_summaries = [tf.summary.histogram(name=\"weight_summary%d\"%i, values=g[1]) for i, g in enumerate(gradients_list)]\n",
    "gradient_summaries = [tf.summary.histogram(name=\"gradient_summary%d\"%i, values=g[0]) for i, g in enumerate(gradients_list)]\n",
    "\n",
    "logging_summary = tf.summary.merge([loss_summary, acc_summary] + hidden_summaries + gradient_summaries + act_summaries)\n",
    "param_summary = tf.summary.merge(weight_summaries)'''\n",
    "\n",
    "batch_size = 128\n",
    "n_batch = int (train_images.shape [0] / batch_size)\n",
    "loss_list = []\n",
    "logging_data_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_name = '%s_Layer%d_Scale%.2f_%s_%s'%(MODE, NUM_LAYER, SCALE, INIT, ACT) + COMMENT\n",
    "dir_name = '%s_Layer%d_Scale%.2f_%s_%s'%(MODE, NUM_LAYER, SCALE, INIT, ACT) + COMMENT\n",
    "if BatchNorm:\n",
    "    fig_name += '_BatchNorm%.2f' %INIT_GAMMA\n",
    "    dir_name += '_BatchNorm%.2f' %INIT_GAMMA\n",
    "if GradientClipping:\n",
    "    fig_name += '_GradientClipping'\n",
    "    dir_name += '_GradientClipping'\n",
    "if WeightClipping:\n",
    "    fig_name += '_WeightClipping'\n",
    "    dir_name += '_WeightClipping'\n",
    "if WeightNormalization:\n",
    "    fig_name += '_WeightNormalization'\n",
    "    dir_name += '_WeightNormalization'\n",
    "if WeightRandomize:\n",
    "    fig_name += '_WeightRandomize'\n",
    "    dir_name += '_WeightRandomize'\n",
    "\n",
    "def plotHAG(epoch, hidden_results, act_results, gradients_results, weight_results, y_grad_results, figname=fig_name, period=4):\n",
    "\n",
    "    h_var_list = [h.var(1).mean() for h in hidden_results]\n",
    "    a_var_list = [a.var(1).mean() for a in act_results]\n",
    "    g_var_list = [g[0].var() for g in gradients_results]\n",
    "    w_var_list = [w.var() for w in weight_results]\n",
    "    y_var_list = [y[0].var(1).mean() for y in y_grad_results]\n",
    "    \n",
    "    fig, ax=plt.subplots(5, 2, figsize=(12,20))\n",
    "\n",
    "    figname += '[%d]'%epoch\n",
    "    fig.suptitle(figname, fontsize=16)\n",
    "    ax[0,0].plot(range(1, NUM_LAYER), np.log10(a_var_list))\n",
    "    ax[0,0].set_ylabel('Forward Activation Variance (log scale)', fontsize=13)\n",
    "    if np.isfinite(np.min(np.log10(a_var_list))):\n",
    "        ax[0,0].set_ylim([np.min(np.log10(a_var_list))-0.1, np.min(np.log10(a_var_list))+11.1])\n",
    "    elif np.isfinite(np.max(np.log10(a_var_list))):\n",
    "        ax[0,0].set_ylim([np.max(np.log10(a_var_list))+0.1, np.max(np.log10(a_var_list))-11.1])\n",
    "    \n",
    "    ax[1,0].plot(range(1, NUM_LAYER), np.log10(h_var_list))\n",
    "    ax[1,0].set_ylabel('Forward Node Variance (log scale)', fontsize=13)\n",
    "    if np.isfinite(np.min(np.log10(h_var_list))):\n",
    "        ax[1,0].set_ylim([np.min(np.log10(h_var_list))-0.1, np.min(np.log10(h_var_list))+11.1])\n",
    "    elif np.isfinite(np.max(np.log10(h_var_list))):\n",
    "        ax[1,0].set_ylim([np.max(np.log10(h_var_list))+0.1, np.max(np.log10(h_var_list))-11.1])\n",
    "    \n",
    "    ax[2,0].plot(range(1, NUM_LAYER), np.log10(y_var_list))\n",
    "    ax[2,0].set_ylabel('Backward Variance (log scale)', fontsize=13)\n",
    "    if np.isfinite(np.min(np.log10(y_var_list))):\n",
    "        ax[2,0].set_ylim([np.min(np.log10(y_var_list))-0.1, np.min(np.log10(y_var_list))+11.1])\n",
    "    elif np.isfinite(np.max(np.log10(y_var_list))):\n",
    "        ax[2,0].set_ylim([np.max(np.log10(y_var_list))+0.1, np.max(np.log10(y_var_list))-11.1])\n",
    "    \n",
    "    ax[3,0].plot(range(3, NUM_LAYER-3), np.log10(g_var_list[3:-3]))\n",
    "    ax[3,0].set_ylabel('Weight Gradients Variance (log scale)', fontsize=13)\n",
    "    if np.isfinite(np.min(np.log10(g_var_list))):\n",
    "        ax[3,0].set_ylim([np.min(np.log10(g_var_list))-0.1, np.min(np.log10(g_var_list))+11.1])\n",
    "    elif np.isfinite(np.max(np.log10(g_var_list))):\n",
    "        ax[3,0].set_ylim([np.max(np.log10(g_var_list))+0.1, np.max(np.log10(g_var_list))-11.1])\n",
    "    \n",
    "    ax[4,0].plot(range(3, NUM_LAYER-3), np.log10(w_var_list[3:-3]))\n",
    "    ax[4,0].set_ylabel('Weight Variance (log scale)', fontsize=13)\n",
    "    if np.isfinite(np.min(np.log10(w_var_list))):\n",
    "        ax[4,0].set_ylim([np.min(np.log10(w_var_list))-0.1, np.min(np.log10(w_var_list))+11.1])\n",
    "    elif np.isfinite(np.max(np.log10(w_var_list))):\n",
    "        ax[4,0].set_ylim([np.max(np.log10(w_var_list))+0.1, np.max(np.log10(w_var_list))-11.1])\n",
    "    ax[4,0].set_xlabel('Layer Index')\n",
    "\n",
    "    ax[0,1].set_ylabel('Forward Activation Histogram')\n",
    "    ax[1,1].set_ylabel('Forward Node Histogram')\n",
    "    ax[2,1].set_ylabel('Backward Histogram')\n",
    "    ax[3,1].set_ylabel('Weight Gradients Histogram')\n",
    "    ax[4,1].set_ylabel('Weight Histogram')\n",
    "    ax[4,1].set_xlabel('Value')\n",
    "    for i in range(1, NUM_LAYER-1, period):\n",
    "        gp = plt.hist(gradients_results[i][0].ravel(), bins=20, color='w')\n",
    "        hp = plt.hist(hidden_results[i].ravel(), bins=20, color='w')\n",
    "        ap = plt.hist(act_results[i].ravel(), bins=20, color='w')\n",
    "        wp = plt.hist(weight_results[i].ravel(), bins=20, color='w')\n",
    "        yp = plt.hist(y_grad_results[i][0].ravel(), bins=20, color='w')\n",
    "        ax[0,1].plot(ap[1][0:-1], ap[0], color=plt.get_cmap('plasma')(i/NUM_LAYER))\n",
    "        ax[1,1].plot(hp[1][0:-1], hp[0], color=plt.get_cmap('plasma')(i/NUM_LAYER))\n",
    "        ax[2,1].plot(yp[1][0:-1], yp[0], color=plt.get_cmap('plasma')(i/NUM_LAYER))\n",
    "        ax[3,1].plot(gp[1][0:-1], gp[0], color=plt.get_cmap('plasma')(i/NUM_LAYER))\n",
    "        ax[4,1].plot(wp[1][0:-1], wp[0], color=plt.get_cmap('plasma')(i/NUM_LAYER))\n",
    "        if i == 1:\n",
    "            ax[4,1].set_ylim([0, max(wp[0])+1])\n",
    "            ax[4,1].set_xlim([wp[1][0]*1.05, wp[1][-1]*1.05])\n",
    "\n",
    "    cax = fig.add_axes([0.95, 0.20, 0.01, 0.65])\n",
    "    s_map = cm.ScalarMappable(norm=mcolors.Normalize(vmin=0, vmax=1), cmap=plt.get_cmap('plasma'))\n",
    "    s_map.set_array([0,1])\n",
    "    cbar = plt.colorbar(s_map, cax=cax, ticks=[0, 1])\n",
    "    cbar.ax.set_yticklabels(['Input', 'Output'])\n",
    "        \n",
    "    fig.savefig('img/VarianceAndHistogram/' + figname + '.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "def reindexCorr(data):\n",
    "    X = np.corrcoef(data.T)**2\n",
    "    d = sch.distance.pdist(X)   # vector of ('55' choose 2) pairwise distances\n",
    "    L = sch.linkage(d, method='complete')\n",
    "    ind = sch.fcluster(L, 0.5*d.max(), 'distance')\n",
    "    new_data = np.concatenate([np.expand_dims(data[:, i], 1) for i in list((np.argsort(ind)))], axis=1)\n",
    "    return new_data.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCov(epoch, hidden_results, figname=fig_name, period=4, reindex=True):\n",
    "    \n",
    "    N = len(hidden_results)//period+1\n",
    "    fig, ax=plt.subplots(\n",
    "        N//2+N%2, 2,\n",
    "        figsize=(12, 6*(N//2+N%2))\n",
    "    )\n",
    "    figname += '[%d]'%epoch\n",
    "    fig.suptitle(figname, fontsize=16)\n",
    "    for i, h in enumerate(hidden_results[0::period]):\n",
    "        if reindex:\n",
    "            ax[i//2, i%2].matshow(np.corrcoef(reindexCorr(h))**2, vmin=0, vmax=1, cmap='plasma')\n",
    "        else:\n",
    "            ax[i//2, i%2].matshow(np.corrcoef(h)**2, vmin=0, vmax=1, cmap='plasma')\n",
    "        ax[i//2, i%2].set_title('Layer %d'%(i*period))\n",
    "        \n",
    "    s_map = cm.ScalarMappable(norm=mcolors.Normalize(vmin=0, vmax=1), cmap=plt.get_cmap('plasma'))\n",
    "    s_map.set_array([0.,1.])\n",
    "    cax = fig.add_axes([0.95, 0.20, 0.01, 0.65])\n",
    "    cbar = plt.colorbar(s_map, cax=cax, ticks=[ 0., 1.0])\n",
    "    cbar.ax.set_yticklabels(['Low correlation', 'High correlation'])\n",
    "    \n",
    "    fig.savefig('img/CorrelationCoefficient/' + figname + '.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SaveJacobian:\n",
    "    jacobians_op = []\n",
    "    jac_layer_period = NUM_LAYER//6\n",
    "    jac_nodes_number = 40\n",
    "    for _a in a_list[0::jac_layer_period]:\n",
    "        jacobians_op += [opt.compute_gradients(_a[:,node], h_list[0]) for node in range(0, jac_nodes_number)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSVD(epoch, jacobians, figname=fig_name):\n",
    "    N = len(jacobians)//jac_nodes_number\n",
    "    fig, ax=plt.subplots(\n",
    "        1, 1, figsize=(6, 6)\n",
    "    )\n",
    "    figname += '[%d]'%epoch\n",
    "    fig.suptitle(figname, fontsize=16)\n",
    "\n",
    "    count = 0\n",
    "    for ith_layer in range(0, NUM_LAYER, jac_layer_period):\n",
    "        one_jac = np.concatenate(jacobians[count*jac_nodes_number:(count+1)*jac_nodes_number], 0)\n",
    "        _, S, _ = np.linalg.svd(one_jac)\n",
    "        ax.plot(S, color=plt.get_cmap('plasma')(ith_layer/NUM_LAYER))\n",
    "        count += 1\n",
    "        \n",
    "    ax.set_xlabel('Singular Value Index', fontsize=13)\n",
    "    ax.set_ylabel('Singular Value', fontsize=13)\n",
    "    \n",
    "    cax = fig.add_axes([0.95, 0.20, 0.01, 0.65])\n",
    "    s_map = cm.ScalarMappable(norm=mcolors.Normalize(vmin=0, vmax=1), cmap=plt.get_cmap('plasma'))\n",
    "    s_map.set_array([0,1])\n",
    "    cbar = plt.colorbar(s_map, cax=cax, ticks=[i/NUM_LAYER for i in range(0, NUM_LAYER, jac_layer_period)]+[1])\n",
    "    cbar.ax.set_yticklabels(['Layer %d'%i for i in range(0, NUM_LAYER, jac_layer_period)]+['Output'])\n",
    "    \n",
    "    fig.savefig('img/SingularValue/' + figname + '.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VDeval(logging_data):\n",
    "    var_f = logging_data['H']\n",
    "    var_b = logging_data['Y']\n",
    "    VD_f = np.log10(max(var_f)/min(var_f))\n",
    "    VD_b = np.log10(max(var_b)/min(var_b))\n",
    "    return max(VD_f, VD_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for ith_epoch in range(n_epochs):\n",
    "        avg_loss = 0\n",
    "        for ith_batch in range(n_batch):\n",
    "            if ith_batch % LOGGING_BATCH == 0:\n",
    "                hidden_results, act_results, gradients_results, y_gradients_results, weight_results = sess.run(\n",
    "                    [h_list, a_list, gradients_list, y_gradients_list, W_list],\n",
    "                    feed_dict={X:test_images , Y:test_labels, isTrain:True}\n",
    "                )\n",
    "                if SaveJacobian:\n",
    "                    jacobians = sess.run([j[0][0] for j in jacobians_op], feed_dict={X:test_images.mean(0, keepdims=True), isTrain:True})\n",
    "                if ith_batch == 0:\n",
    "                    print('Start plotHAG %d'%ith_epoch)\n",
    "                    plotHAG(ith_epoch, \n",
    "                            hidden_results, act_results, gradients_results, weight_results, y_gradients_results,\n",
    "                            fig_name+'[%d]'%ith_epoch, period=NUM_LAYER//7\n",
    "                    )\n",
    "                    print('Start plotCOV %d'%ith_epoch)\n",
    "                    plotCov(ith_epoch, hidden_results, fig_name, period=NUM_LAYER//7)\n",
    "                    if SaveJacobian:\n",
    "                        plotSVD(ith_epoch, jacobians)\n",
    "                    \n",
    "                logging_data_list.append({\n",
    "                    'Epoch':ith_epoch + ith_batch/n_batch,\n",
    "                    'H': [h.var() for h in hidden_results],\n",
    "                    'Y': [y[0].var() for y in y_gradients_results],\n",
    "                })\n",
    "                print('Epoch %.2f is done.' %(ith_epoch + ith_batch/n_batch))\n",
    "            \n",
    "            X_batch = train_images [ith_batch * batch_size :ith_batch * batch_size + batch_size]\n",
    "            Y_batch = train_labels [ith_batch * batch_size :ith_batch * batch_size + batch_size]\n",
    "\n",
    "            loss_value, _ = sess.run ([loss , update_weights] , feed_dict = {X:X_batch , Y:Y_batch, isTrain:True})\n",
    "            avg_loss += loss_value / n_batch\n",
    "            loss_list.append(avg_loss)\n",
    "\n",
    "        print('Epoch {0} : loss {1}'.format (ith_epoch + 1 , avg_loss))\n",
    "        \n",
    "    print('Acc : ' , acc.eval({X:test_images , Y:test_labels, isTrain:True}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_n = 6\n",
    "_l = 50\n",
    "_stride = 80\n",
    "fig, ax = plt.subplots(_n, _n, figsize=(20,20))\n",
    "fig.suptitle('Gradients of Pairwise Neurons in Layer%d on Testing Data'%_l, fontsize=20)\n",
    "for _x in range(_n):\n",
    "    for _y in range(_n):\n",
    "        ax[_x][_y].plot(y_gradients_results[_l][0][_x*_stride,:], y_gradients_results[_l][0][_y*_stride,:], '.')\n",
    "        if _y == 0:\n",
    "            ax[_x][_y].set_ylabel('Gradient of neuron %d'%(_x*_stride))\n",
    "        if _x == _n-1:\n",
    "            ax[_x][_y].set_xlabel('Gradient of neuron %d'%(_y*_stride))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_data_list[0]['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.plot(\n",
    "    [ld['Epoch'] for ld in logging_data_list],\n",
    "    [VDeval(ld) for ld in logging_data_list]\n",
    ")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Actual Virtual Depth')\n",
    "plt.title('Virtual Depth to Epoch')\n",
    "fig.savefig('img/VirtualDepth/' + fig_name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 199, 10):\n",
    "    sp = plt.hist(np.linalg.svd(weight_results[i])[1], bins=20, color='w')\n",
    "    plt.plot(sp[1][0:-1], sp[0], color=plt.get_cmap('plasma')(i/NUM_LAYER))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, test_images[i].mean(), test_images[i].var()) for i in range(0, 10000, 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(test_images)*0.1, np.min(test_images)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
